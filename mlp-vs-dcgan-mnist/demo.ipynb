{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MLP vs DCGAN on MNIST\n",
        "This notebook compares the performance of two GAN architectures — a fully connected (MLP-based) GAN and a convolutional DCGAN — on the MNIST dataset.\n",
        "\n",
        "We'll evaluate each model's training behavior and visualize generated samples over time.\n"
      ],
      "metadata": {
        "id": "sSiP2nEO9zuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from models import GAN_NN_MNIST, DCGAN_MNIST\n",
        "from utils import random_noise, dcgan_random_noise, train_gan_model, plot_generated_images\n"
      ],
      "metadata": {
        "id": "_L5ZAh3H-Cag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Seed & MLP-GAN Config**"
      ],
      "metadata": {
        "id": "m6bnYVvb-IVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# MLP-GAN Hyperparameters\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "latent_dim = 100\n",
        "z_mode = 'uniform'\n",
        "image_size = (28, 28)\n",
        "input_channel = 1\n",
        "n_filters = 32\n",
        "hidden_units = 256\n",
        "num_layers = 3\n"
      ],
      "metadata": {
        "id": "yqazRbds-E70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load MNIST Dataset\n",
        "We apply a standard normalization transform to scale pixel values to [-1, 1] range for use with Tanh activation.\n"
      ],
      "metadata": {
        "id": "TOe9VsHi-Sut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "\n",
        "mnist_dataset = torchvision.datasets.MNIST(root='./', train=True, download=True, transform=transform)\n",
        "train_dl = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "TDQHtYxQ-WEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train MLP-GAN\n",
        "We use a fully connected generator and discriminator with 3 layers each.\n"
      ],
      "metadata": {
        "id": "z6oRVrB4-cFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_nn = GAN_NN_MNIST(\n",
        "    input_size=latent_dim, gen_hidden_units=hidden_units, gen_num_layers=num_layers,\n",
        "    gen_output_size=784, disc_hidden_units=hidden_units, disc_num_layers=num_layers,\n",
        "    disc_output_size=1, dropout=0.1).to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "gen_optimizer_nn = torch.optim.Adam(model_nn.gen_model.parameters(), lr=1e-3)\n",
        "disc_optimizer_nn = torch.optim.Adam(model_nn.disc_model.parameters(), lr=1e-3)\n",
        "\n",
        "fixed_z = random_noise(batch_size, latent_dim, z_mode).to(device)\n",
        "training_progress_check = random_noise(16, latent_dim, z_mode)\n",
        "\n",
        "all_g_losses, all_d_losses, epoch_samples = train_gan_model(\n",
        "    model_nn, gen_optimizer_nn, disc_optimizer_nn, loss_fn,\n",
        "    num_epochs, train_dl, latent_dim, training_progress_check,\n",
        "    fixed_z, model_type='nn_gan'\n",
        ")\n"
      ],
      "metadata": {
        "id": "sBozLkSy-eOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot MLP-GAN Losses**"
      ],
      "metadata": {
        "id": "ihae3nhV-ruX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_losses_cpu = [g.cpu() if torch.is_tensor(g) else g for g in all_g_losses]\n",
        "half_d_losses = [d.cpu()/2 if torch.is_tensor(d) else d/2 for d in all_d_losses]\n",
        "\n",
        "plt.plot(g_losses_cpu, label='Generator loss')\n",
        "plt.plot(half_d_losses, label='Discriminator loss')\n",
        "plt.legend(fontsize=20)\n",
        "plt.xlabel('Epoch', size=15)\n",
        "plt.ylabel('Loss', size=15)\n",
        "plt.title(\"MLP-GAN\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XymW-_6Q-u5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Show MLP Images by Epoch**"
      ],
      "metadata": {
        "id": "uSACGom5-0su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_epochs = [1, 2, 25, 42, 48, num_epochs]\n",
        "fig = plt.figure(figsize=(10, 14))\n",
        "plt.suptitle(\"MLP-GAN: Sample Images Created During Training\", fontsize=12, color='blue', y=0.92)\n",
        "\n",
        "for i, e in enumerate(selected_epochs):\n",
        "    for j in range(5):\n",
        "        ax = fig.add_subplot(6, 5, i*5 + j + 1)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if j == 0:\n",
        "            ax.text(-0.06, 0.5, f'Epoch {e}', rotation=90, size=18, color='red',\n",
        "                    horizontalalignment='right', verticalalignment='center', transform=ax.transAxes)\n",
        "        image = epoch_samples[e - 1][j]\n",
        "        ax.imshow(image, cmap='gray_r')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qjlK8KRu_AB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLP Final Output Grid**"
      ],
      "metadata": {
        "id": "_xeQE04g_GJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_generated_images(model_nn.gen_model, grid_dim=20, latent_dim=latent_dim, model_type='nn_gan')\n"
      ],
      "metadata": {
        "id": "PmwRSsNh_IWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train DCGAN\n",
        "Next, we use a convolutional generator and discriminator, following the DCGAN design pattern.\n"
      ],
      "metadata": {
        "id": "cWBkEwBJ_Ml5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DCGAN Setup & Training"
      ],
      "metadata": {
        "id": "3I_0rpeA_SXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DCGAN hyperparameters\n",
        "latent_dim = 100\n",
        "n_filters = 64\n",
        "batch_size = 32\n",
        "\n",
        "mnist_dataset = torchvision.datasets.MNIST(root='./', train=True, download=True, transform=transform)\n",
        "train_dl = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "model_cnn = DCGAN_MNIST(latent_dim, n_filters, dropout=0.1).to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "gen_optimizer_dc = torch.optim.Adam(model_cnn.gen_model.parameters(), lr=1e-3)\n",
        "disc_optimizer_dc = torch.optim.Adam(model_cnn.disc_model.parameters(), lr=1e-3)\n",
        "\n",
        "fixed_z = dcgan_random_noise(batch_size, latent_dim, z_mode).to(device)\n",
        "training_progress_check = dcgan_random_noise(16, latent_dim, z_mode)\n",
        "\n",
        "all_g_loss_cnn, all_d_loss_cnn, epoch_samples_cnn = train_gan_model(\n",
        "    model_cnn, gen_optimizer_dc, disc_optimizer_dc, loss_fn,\n",
        "    num_epochs, train_dl, latent_dim, training_progress_check,\n",
        "    fixed_z, model_type='dc_gan'\n",
        ")\n"
      ],
      "metadata": {
        "id": "g-tUncX6_NwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DCGAN Loss Plots**"
      ],
      "metadata": {
        "id": "86nmK9bc_ZmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_losses_cpu_cnn = [g.cpu() if torch.is_tensor(g) else g for g in all_g_loss_cnn]\n",
        "half_d_loss_cnn = [d.cpu()/2 if torch.is_tensor(d) else d/2 for d in all_d_loss_cnn]\n",
        "\n",
        "plt.plot(g_losses_cpu_cnn, label='Generator loss')\n",
        "plt.plot(half_d_loss_cnn, label='Discriminator loss')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"CNN-GAN\")\n",
        "plt.xlabel('Epoch', size=15)\n",
        "plt.ylabel('Loss', size=15)\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GjmRYFpY_c3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DCGAN Sample Images**"
      ],
      "metadata": {
        "id": "R6hC7AML_jx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_epochs = [1, 2, 25, 42, 48, num_epochs]\n",
        "fig = plt.figure(figsize=(10, 14))\n",
        "plt.suptitle(\"CNN-GAN: Sample Images Created During Training\", fontsize=12, color='blue', y=0.92)\n",
        "\n",
        "for i, e in enumerate(selected_epochs):\n",
        "    for j in range(5):\n",
        "        ax = fig.add_subplot(6, 5, i*5 + j + 1)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if j == 0:\n",
        "            ax.text(-0.06, 0.5, f'Epoch {e}', rotation=90, size=18, color='red',\n",
        "                    horizontalalignment='right', verticalalignment='center', transform=ax.transAxes)\n",
        "        image = epoch_samples_cnn[e - 1][j]\n",
        "        ax.imshow(image, cmap='gray_r')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8KLEET2e_l98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DCGAN Final Grid**"
      ],
      "metadata": {
        "id": "56BII7MZ_qqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_generated_images(model_cnn.gen_model, grid_dim=20, latent_dim=latent_dim, model_type='cnn_gan')\n"
      ],
      "metadata": {
        "id": "5fsaOygf_vV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Conclusion\n",
        "\n",
        "Both GANs are capable of learning to generate handwritten digits from the MNIST dataset.\n",
        "\n",
        "- **MLP-GAN** converges reasonably but often produces less sharp images.\n",
        "- **DCGAN** learns spatial features better and generates more realistic digits earlier in training.\n",
        "\n",
        "If image quality is a priority, **DCGAN** is the better architecture due to its inductive bias for images.\n",
        "\n",
        "You've now compared two core GAN architectures side-by-side on MNIST. Feel free to tweak hyperparameters or test on new datasets!\n"
      ],
      "metadata": {
        "id": "PEoiTntH_6u9"
      }
    }
  ]
}